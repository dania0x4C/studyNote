
# **Rdd**

**Rdd생성**

```python
import os, sys
import pyspark

#os.environ["PYSPARK_PYTHON"]=sys.executable          
#os.environ["PYSPARK_DRIVER_PYTHON"]=sys.executable

myConf=pyspark.SparkConf() #여기에 필요한 설정을 정의한다
#myConf=pyspark.SparkConf().set("spark.driver.bindAddress", "127.0.0.1")
#myConf=pyspark.SparkConf().set("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.13:10.1.1") 
spark = pyspark.sql.SparkSession\
    .builder\
    .master("local")\
    .appName("myApp")\
    .config(conf=myConf)\
    .getOrCreate()
```

**파일 Rdd로 받아오기**

```python
myRdd=spark.sparkContext\
    .textFile(os.path.join("data","파일"), use_unicode = True)
```

**리스트 Rdd로 받아오기**

```python
rdd=spark.sparkContext.parallelize(List)
```

**Rdd 함수**

1. map: x요소를 반복 사용
2. flatmap: 이거랑 map이랑 차이점 확인하기
3. reduce: x, y를 이용해서 y에 누적 계산
4. fold: 파티션 별로 나누어서 reduce랑 비슷하게 동작(근데 사용 안 할듯)
5. filter: filter로 걸러서 원하는 값을 반환
6. foreach: 반환 값은 없고 각 요소에 대해서 계산 적용하기만 한다

**unparied - groupby**

**paired Rdd**

1. mapValue(): value에 대한 map을 진행
2. groupByKey(): key를 기준으로 집단화해서 모아둠
3. reduceByKey(): key가 같은 값을 partition별로 계산 함
4. sortByKey(): value를 기준으로 오름, 내림으로 정렬
5. countByKey(): key별로 계산
6. combineByKey: 데이터를 특정 키 기준으로 집단화하고 집단별 작업 수행

사실 이해가 안됨

```python
Rdd.combineByKey(lambda value: (value,1), # combiner 각 키별 튜플을 만듬
                     lambda x,value: (x[0]+value, x[1]+1), # merge values 동일 키의 값을 더함
                     lambda x,y: (x[0]+y[0], x[1]+y[1])) # partition별로 combiner을 더함
```

**그래프 나중에 많이 나옴**

# **DataFrame**

**DataFrame생성**

```python
spark.createDataFrame(데이터리스트, 스키마)
```

1. **Rdd에서 생성**

```python
Rdd = spark.sparkContext.parallelize(리스트모음)

rddDf1=spark.createDataFrame(Rdd, 스키마)
```

1. **schema 생성**

```python
from pyspark.sql.types import *

schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("height", DoubleType(), True)
])

```

pandas로 csv파일 사용

```python
_myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))

import pandas as pd
icc = pd.DataFrame( { 'country': ['South Korea','Japan','Hong Kong'],'codes': [81, 82, 852] })
```

1. row로 생성
2. **csv 파일에서 생성**

```python
df = spark\
        .read\
        .format('csv')\
        .options(header='true', inferschema='true', delimiter=',')\
        .load(os.path.join('data','파일명'))

#TRUE는 첫 헤더 유무
```

1. tsv파일 일기

1. **json파일 읽기**

```python
import os
jfile= os.path.join('src','ds_twitter_seoul_3.json')

tweetDf= spark.read.json(jfile)
```

1. parquet 파일 읽기

1. datetime

```python
from pyspark.sql.functions import udf ##함수를 등록할때 udf를 사용해서 사용함
from pyspark.sql.types import DateType
toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())##DateType() 이게 반환하는 타입임임
```

1. **DataFrame API**

```python
*from* pyspark.sql *import* functions *as* F

#컬럼명 변경 및 데이터형태 바꾸기
tDf = tDf.withColumn("weight", tDf['_c2'].cast("double"))# 그냥
tDf = tDf.withColumn("id", F.col('_c0').astype("integer"))# f함수

alias: 컴럼명 변경
```

1. **사용자 정의 함수 udf** - 조건에 따른 withColumn 가능
2. 기술통계 -  리스트.describe()
3. **그래프**

1. plot(점)or line
2. boxplot
3. violinplot
4. bar

DataFrame 값 조회 aggregate fuctions - F함수도 있음

컬럼 내용 변경

```
from pyspark.sql.functions import *

_heightDf = myDf.withColumn('nameNew', regexp_replace('name', 'lee', 'lim'))
## lee를 lim으로 변경해달라
_heightDf.show()
```

**결측값 - null에 0을 넣든 평균 값을 넣든 하면 됨**

```python
from pyspark.sql.functions import isnan, when, count, col
myDf.select([count(when(col(c).isNull(), c)).alias(c) for c in myDf.columns]).show()
```

**분기나누기**

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def classifyQuarter(s):
    q=""
    s=int(s)
    if 1<=s and s< 4:
        q="Q1"
    elif 4<=s and s<7:
        q="Q2"
    elif 7<=s and s<10:
        q="Q3"
    elif 10<=s and s<=12:
        q="Q4"
    else:
        q="no"
    return q

quarter_udf = udf(classifyQuarter, StringType())
```

pandas pivot

- dataframe 값 그래프로 만드는 과정

**spark sql - 실제로 sql문을 사용하는 방법인거 같음 확인 필요**

**mongo Spark connector 해보기**

# **Statistic**

1. sampling
    1. random(행, 열)
    
2. 균등분포
    1. hist - 그래프
    
3. 정규분포
    1. normal(평균, 표준편차, size = n)
    2. histogram() - 2개의 행렬 반환
    3. concatenate - 2개의 그래프 합치기

1. 중심극한정리

```python
import numpy as np
x=np.random.randn(100)

import pandas as pd
xPd=pd.DataFrame(x, columns=['x'], dtype=float)

from pyspark.sql import functions as f
df.select(f.skewness(df['x']), f.kurtosis(df['x'])).show()
```

1. 분산 계산 - np로 계산 하는게 빠를듯

1. zscore, tsocre

```python
from scipy import stats

1. stats.zscore(X)

2. cdf(x, loc = 0, scale = 1) - 누적확률(찾고자하는 지점, 평균, 표준편차)

3. ppf(x, loc = 0, scale = 1) - 확률의 점 계산(찾고자하는 지점, 평균, 표준편차)

4. t.cdf(1, 1, loc=0, scale=1)# cdf에 해당하는 t값, 자유도, 평균, 표준편차
```

1. outlier??????
    
    
    window fuction: 컬럼에 대해서 그룹을 만들기
    
    1. 순위 함수: row_number()
    2. ,, :rank()
    3. 누적 분포 값: cume_dist()
    4. lag, lead 함수
    5. F 함수: sum, min, max

1. 추론
    
    Kolmogorov-Smirnov
    

```python
from pyspark.mllib.stat import Statistics

testResult = Statistics.kolmogorovSmirnovTest(Rdd, "norm")

print(testResult)
```

Anova(fscore도 있음)

```python
import statsmodels.api as sm

moore=sm.datasets.get_rdataset("Moore","carData")
##carData라는 package에서 Moore라는 데이터세트를 가져옴

from statsmodels.formula.api import ols

formula='fscore~C(status)*C(fcategory)'
model=ols(formula, data=moore.data).fit()

from statsmodels.stats.anova import anova_lm

result = anova_lm(model)
```

1. 빈도분석 - 명목변수 빈도 계산

```python
crosstab - 교차표, NULL은 0으로 바뀜

freqItems

# 50프로 이상 발생한 행 출력
freq = coffeeDf.stat.freqItems(["name","coffee"], 0.5)

ChiSquareTest 

#features - vector형태로 들어가야함
from pyspark.ml.stat import ChiSquareTest
r = ChiSquareTest.test(_coffeeDf, "features", "label")
```

1. 상관관계

```markdown
* + 변수가 서로 같은 방향으로 변동, 숫자가 클수록 강도가 크며 반대로 적을수록 적다.
* 0 변수가 서로 영향을 주지 않음
* - 변수가 서로 다른 방향으로 변동
```

1. scatter 
    
    상관관계 그래프 산포
    

1. 공분산 구하기

1. 상관관계 구하기

1. 리스트로 상관관계 구하기

```python
import numpy as np

xbar=np.mean(X)
ybar=np.mean(Y)
sx=np.sqrt(np.var(X,ddof=1))
sy=np.sqrt(np.var(Y,ddof=1))
covxy=sum( [ (x-xbar)*(y-ybar) for x,y in zip(X,Y) ]) / (len(X)-1)

rxy=covxy/(sx*sy)
print ("corr: ",rxy)
```

1. zscore로 상관관계 구하기 (zscore를 이용한 상관관계)

1. 12.8 scipy   상관관계

1. spark
    
    ```python
    from pyspark.sql.functions import rand
    
    df = spark.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))
    
    print (df.stat.corr('rand1', 'rand2'))
    ```
    

1. 상관관계 표

```python
import numpy as np

a=np.array([0.1, .32, .2, 0.4, 0.8])
b=np.array([.23, .18, .56, .61, .12])
c=np.array([.9, .3, .6, .5, .3])
d=np.array([.34, .75, .91, .19, .21])

np.corrcoef([a,b,c,d])
```

# **데이터 변환**

mllib - Rdd

ml - DataFrame

1. Rdd변환
    1. vector
        1. dense
            
            ```python
            Rdd
            from pyspark.mllib.linalg import Vectors
            
            dv1mllib=Vectors.dense([1.0, 2.1, 3])
            
            DataFrame
            from pyspark.ml.linalg import Vectors
            
            dv1ml=Vectors.dense([1.0, 2.1, 3])
            ```
            
        2. sparse
            
            ```python
            # 희소행렬 - # 5개의 컬럼-> 0, 1, 4번째에 값 존재
            
            sv1 = Vectors.sparse(5,[0,1,4],[160.0,69.0,24.0])
            
            sv1.toArray()
            ```
            
    2. Matrix
        
        dense
        
        ```python
        from pyspark.mllib.linalg import Matrices
        #행 3개 열 2개
        Matrices.dense(3, 2, [1,2,3,4,5,6])
        # 1 4
        # 2 5
        # 3 6
        ```
        
        sparse
        
        ```python
        #열기반 희소행렬
        from scipy.sparse import csc_matrix# 컬럼별로 읽기
        
        sparse_csc = csc_matrix([[1, 0, 2],
                                 [0, 0, 3],
                                 [4, 5, 6]])
        print(sparse_csc)
        
        #1-> 4-> 5-> 2-> 3-> 6
        
        from scipy.sparse import csr_matrix # ROW별로 읽기
        
        sparse_csr = csr_matrix([[1, 0, 2],
                                 [0, 0, 3],
                                 [4, 5, 6]])
        print(sparse_csr)
        
        #1-> 2-> 3-> 4-> 5-> 6
        ```
        
        rowMatrix
        
        Rdd → labelPoint
        
        ```python
        # mllib
        from pyspark.mllib.regression import LabeledPoint
        
        LabeledPoint(1.0, dv1mllib)
        
        # ml
        from pyspark.mllib.regression import LabeledPoint
        from pyspark.mllib.linalg import Vectors
        
        LabeledPoint(1.0, Vectors.fromML(dv1ml))
        ```
        
    3. TF-IDF
        1. tf - rdd 단어빈도 계산 fit 사용 x
        2. tf-idf dataFrame 단어빈도 계산 fit 사용 o
    4. StandardScaler(Rdd)
        
        ```python
        from pyspark.mllib.feature import StandardScaler
        scaler1 = StandardScaler().fit(tRdd)
        scaler2 = StandardScaler(withMean=True, withStd=True).fit(tRdd)
        ```
        
2. DataFrame 변환
    1. Labeled Point에서 dataFrame 생성
        
        ```python
        # 1번
        from pyspark.mllib.regression import LabeledPoint
        p = [LabeledPoint(1, [1.0,2.0,3.0]),
             LabeledPoint(1, [1.1,2.1,3.1]),
             LabeledPoint(0, [1.2,2.2,3.3])]
        
        trainDf=spark.createDataFrame(p)
        
        # 2번
        from pyspark.mllib.linalg import Vectors
        trainDf = spark.createDataFrame([
            (1.0, Vectors.dense([0.0, 1.1, 0.1])),
            (0.0, Vectors.dense([2.0, 1.0, 1.0])),
            (0.0, Vectors.dense([2.0, 1.3, 1.0])),
            (1.0, Vectors.dense([0.0, 1.2, 0.5]))], ["label", "features"])
        
        #3번 Rdd로 만들기?
        ```
        
    2. transformer/ estimator???
        
        
    3. reg tokenizer
    
    ```python
    # reg tokenizer
    # 정규식 기준으로 나누기
    # \s - 공백문자
    # \w - 숫자 및 대소문자(영어만) - 이거 사용법 알아오기
    # * 0또는 이상 값  + 1또는 이상 값
    from pyspark.ml.feature import RegexTokenizer
    
    re = RegexTokenizer(inputCol="sent", outputCol="wordsReg", pattern="\\s+")# \공백문자+ - 한글자 이상
    
    reDf=re.transform(myDf)
    reDf.show()
    ```
    
    1. stopwords
    
    ```python
    # 불용어(DataFrame) 위에서 나눈걸로 filter 한거임
    
    from pyspark.ml.feature import StopWordsRemover
    stop = StopWordsRemover(inputCol="wordsReg", outputCol="nostops")
    
    _mystopwords=[u"나",u"너", u"우리"]
    for e in _mystopwords:
        stopwords.append(e)
    stop.setStopWords(stopwords)
    
    #for e in stop.getStopWords():
     #   print (e, end="/")
    
    #위에서 불용어를 추가하고 마지막에 제거하는 것
    stopDf=stop.transform(reDf) # 제거
    stopDf.show()
    ```
    
    e. CountVec
    
    ```python
    # spark countVectorizer 단어 빈도수 check
    
    from pyspark.ml.feature import CountVectorizer
    cv = CountVectorizer(inputCol="nostops", outputCol="cv", vocabSize=30, minDF=1.0) #cv라고 새로운 틀 만들고
    #minDF = 1.0 문서 1개 이하의 단어는 무시하라
    
    #쨋든 stopDf를 가지고 cv시키기
    cvModel = cv.fit(stopDf) # 새로운 틀 적용
    
    #여기의 의미가 뭐
    cvDf = cvModel.transform(stopDf) #값 변환
    
    cvDf.show(3)
    #(빈도, [위치])
    ```
    
    TF - dataFrame
    
    ```python
    # IDF - spark
    
    from pyspark.ml.feature import HashingTF, IDF
    
    # hashTF = HashingTF(inputCol="nostops", outputCol="hash", numFeatures=32) #  mapping indices insufficient
    hashTF = HashingTF(inputCol="nostops", outputCol="hash")
    
    hashDf = hashTF.transform(stopDf)
    
    hashDf.select("nostops", "hash").show(truncate=False)
    
    # TF - IDF
    idf = IDF(inputCol="hash", outputCol="idf")
    
    idfModel = idf.fit(hashDf)
    
    idfDf = idfModel.transform(hashDf)
    
    for e in idfDf.select("nostops","hash").take(10):
        print(e)
    ```
    
    Word2Vec: 단어의 문맥을 표현
    
    ```python
    #단어벡터를 3개로 나눈다, 최소단어빈도
    from pyspark.ml.feature import Word2Vec
    
    word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="words", outputCol="w2v")
    ```
    
    NGram: n개의 token으로 만들어줌
    
    ```python
    #2개의 token으로 이루어진 순열
    from pyspark.ml.feature import NGram
    
    ngram = NGram(n=2, inputCol="words", outputCol="ngrams")
    ```
    
    StringIndexer: 빈도가 높은 순서로 인덱스 할당
    
    ```python
    from pyspark.ml.feature import StringIndexer
    
    labelIndexer = StringIndexer(inputCol="sent", outputCol="sentLabel")
    ```
    
    one-Hot Encoding: 값을 이진 값으로 표현
    
    ```python
    from pyspark.ml.feature import OneHotEncoder
    
    encoder = OneHotEncoder(inputCol="gradeIndex", outputCol="gradeVec")
    ```
    
    (백터의 길이, [값이 0이 아닌 값의 위치],[0이 아닌 실제 값])
    
    연속데이터 변환: *threshould - 넘으면 1 안 넘으면 0*
    
    ```python
    from pyspark.ml.feature import Binarizer
    
    binarizer = Binarizer(threshold=68.0, inputCol="weight", outputCol="weight2")
    
    from pyspark.ml.feature import QuantileDiscretizer
    
    discretizer = QuantileDiscretizer(numBuckets=3, inputCol="height", outputCol="height3")
    ```
    
    VectorAssembler: 열을 묶어서 vector 열로 만든다
    
    ```python
    from pyspark.ml.feature import VectorAssembler
    
    va = VectorAssembler(inputCols=["weight2","height3"],outputCol="features")
    ```
    
    PipeLine: 여러 estimator를 묶어서 반환
    
    ```python
    from pyspark.ml import Pipeline
    
    pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
    ```