```
import numpy as np
import matplotlib.pyplot as plt

states = {0: 'cool', 1: 'warm', 2:'overheated'}
actions = {0: 'slow', 1: 'fast'}

P = {}

P['slow'] = np.array(
    [[1.0, 0.0, 0.0],
     [0.5, 0.5, 0.0],
     [0.0, 0.0, 1.0]])

P['fast'] = np.array(
    [[0.5, 0.5, 0.0],
     [0.0, 0.0, 1.0],
     [0.0, 0.0, 1.0]])

R = {}

R['slow'] = np.array(
    [[1.0, 0.0, 0.0],
     [1.0, 1.0, 0.0],
     [0.0, 0.0, 0.0]])

R['fast'] = np.array(
    [[2.0, 2.0, 0.0]
     [0.0, 0.0, -10],
     [0.0, 0.0, 0.0]])

  

# update one time

def value_update(values, gamma):

    action_values = np.array([[0.0, 0.0],
                              [0.0, 0.0],
                              [0.0, 0.0]])

    for s in states:
        for col, a in actions.items(): # for each action a, (col is 0, 1; a is "slow", "fast")

            action_values[s, col] = 0.0
            # [(0, 'slow'), (1, 'fast')]

            for next_s in states: # for each next state next_s,
                action_values[s, col] += P[a][s, next_s] * (R[a][s, next_s] + gamma * values[next_s])
                # next_S에 대한 value값 = values[next_S], 그리고 "누적 = 시그마"임

    # synchronous backup
    values = np.max(action_values, axis=1)

    # 그 단계에서 구했던 가장 큰 action_value
    return values, action_values# q값

  
# iterate 20 times with gamma=0.5
gamma = 0.5# 1.0
values = [0.0, 0.0, 0.0]
print(f'initial values={values}')

for k in range(1, 21):
    values, action_values = value_update(values, gamma)
    print(f'k={k}, values={values}')
'''

optimal policy

PI(0) = FAST
PI(1) = SLOW
PI(2) = Undetermined

'''

gamma = 0.5
epsilon = 1e-5
values = [0.0, 0.0, 0.0]
print(f'initial values={values}')

list_diff = []

k = 0

while True:
    k += 1
    old_values = values
    values, action_values = value_update(values, gamma)
    diff = np.linalg.norm(values - old_values) # vector값의 차이
    list_diff.append(diff)

    print(f'k={k}, values={values}, diff={diff:.5f}')

    if diff < epsilon: # 여기가 제일 이해 안 됨
        break  
```