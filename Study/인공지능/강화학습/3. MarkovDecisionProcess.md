# MDP

```
import numpy as np

# 1. 다른 state로 갈때의 percent를 작성한다.

state_index = {0:'Home', 1: 'Coffee', 2: 'Chat', 3: 'Computer'}

PL = [[0.0, 0.9, 0.1, 0.0],
      [0.0, 1.0, 0.0, 0.0],
      [0.0, 0.0, 0.0, 1.0],
      [0.0, 0.0, 0.0, 1.0]]

PR = [[0.0, 0.1, 0.9, 0.0],
      [0.0, 1.0, 0.0, 0.0],
      [0.0, 0.0, 0.0, 1.0],
      [0.0, 0.0, 0.0, 1.0]]
     
R = [[1.0, 1.0, 0.0, 0.0],
     [0.0, 1.0, 2.0, 3.0],
     [0.0, 1.0, -1.0, 2.0],
     [2.0, 1.0, -3.0, 5.0]]

def policy(p_left):
    prob = np.random.uniform()
    
    if prob <= p_left:
        action = 'L'
    else:
        action = 'R'
    return action
```

```
def sample_MDP(p_left, gamma, num_episodes=20, max_len=1000):

    list_episodes = []
    list_action = []
    list_rewards = []
    list_return = []

    for i in range(num_episodes):
        # sample action from policy and decides P matrix
        a = policy(p_left)
        P = PL if a=='L' else PR
        # find terminal states from P
        terminal_states = []
        for row in range(len(P)):
            if P[row][row] == 1.0:
                terminal_states.append(row)
        # start of a new episode
        s = 0 # initial state
        episode = [s]
        reward = []
        G = 0.0 # initialize return to 0 for each episode
        t = 0 # episode counter (time)

        while (t < max_len) and (s not in terminal_states):
            next_s = next_state(s, P)
            r = gamma**t * R[s][next_s] # discounted reward at time t
            G += r
            episode.append(next_s)
            reward.append(r)
            s = next_s # 다음 상태를 현재 상태로 업데이트
            t += 1

        list_episodes.append(episode)
        list_action.append(a)
        list_rewards.append(reward)
        list_return.append(G)

    return list_episodes, list_action, list_rewards, list_return
```
