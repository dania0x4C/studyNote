# MRP
```
import numpy as np

# 1. 다른 state로 갈때의 percent를 작성한다.

state_index = {0:'Home', 1: 'Coffee', 2: 'Chat', 3: 'Computer'}

P = [[0.6, 0.4, 0.0, 0.0],
     [0.0, 0.1, 0.7, 0.2],
     [0.0, 0.2, 0.5, 0.3],
     [0.2, 0.2, 0.1, 0.5]]
     
R = [[1.0, 1.0, 0.0, 0.0],
     [0.0, 1.0, 2.0, 3.0],
     [0.0, 1.0, -1.0, 2.0],
     [2.0, 1.0, -3.0, 5.0]]


# 2. 아래의 배열의 형태로 0~1 사이의 값이 선택 될 수 있게 만들어준다.

cs_p = np.cumsum(P, axis=1)# 0~1의 범위를 이용한 P 추출 -> 누적 합
zero_vec = np.zeros((4,1))# 4행의 1열인 0으로 구성된 matrix 생성
cs_p = np.concatenate((zero_vec, cs_p), axis=1)
'''
[[0. , 0.6, 1. , 1. , 1. ],
 [0. , 0. , 0.1, 0.8, 1. ],
 [0. , 0. , 0.2, 0.7, 1. ],
 [0. , 0.2, 0.4, 0.5, 1. ]]

ex) 값이 0<P<=0.6 값이 나오면 0 state의 값이 나온 것이고 0.6<P<=1.0이 나오면 1 state의 값이 나온 것이다.

'''

# 3. 문제 상황에 따른 애피소드와 그 애피소드를 몇번 반복 할지 정한다.

state_episodes_length = 7# 이론적으로 한 state는 무한적으로 생성되어 하나의 값으로 수렴한다
state_episodes = []
state_episode_length = 24
reward_episodes = []
Gs = []# return

# 4. 한 episode에서 각 state에서 다음 state로 변하는 것을 판단

  

def next_state(s):

    prob = np.random.uniform()
    state_count = len(P)# 행의 갯수

    for change_state in range(state_count):
        if (prob >= cs_p[s][change_state]) and (prob < cs_p[s][change_state+1]):
# 이거 0보다 크고 0.6보다 작다는 의미 그리고 ns값만 바꾸니까
            next_s = change_state
            break
    return next_s

# 5. episode를 여러번 진행하여 배열에 넣음

for i in range(state_episodes_length):

    # start of a new episode
    state_episode = []
    reward_episode = []
    G = 0.0 # initialize return to 0 for each episode
    state = 0# init_state
    gamma = 0.5

# 6. 한 episode를 진행해서 각 state의 변화를 배열에 넣음

    for t in range(state_episode_length):
        next_s = next_state(state)
        r = gamma**t * R[state][next_s] # 보상이 시간에 따라 줄어듬
        G += r

        state_episode.append(next_s)
        reward_episode.append(r)
        state = next_s # 다음 상태를 현재 상태로 업데이트

    state_episodes.append(state_episode)
    reward_episodes.append(reward_episode)
    Gs.append(G)

state_episodes
```

```
# 1. 다음은 p의 값이 1.0인 것은 terminal_state로 더 이상 변하지 않는 state 상태이다.
P = [[1.0, 0.0, 0.0, 0.0],
     [0.0, 0.1, 0.7, 0.2],
     [0.0, 0.2, 0.5, 0.3],
     [0.2, 0.2, 0.1, 0.5]]  

state_episode_length = 100000 # 최대 에피소드 길이

for i in range(num_episodes):
    # start of a new episode
    episode = []
    s = 3 # current_state(computer)
    t = 0 # episode_counter(time)

    while (t < state_episode_length) and (s != 0):
        s = next_state(s)# 다음 상태를 다시 s로 업데이트
        episode.append(s)
        t += 1
    list_episodes.append(episode)
    
list_episodes
```